{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import tensorflow\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#loading data and opening out input data inthe form of a text file\n",
    "#Project Gutenburg/berg is where the data can be found(Just Google it:)\n",
    "file = open (\"frankenstein-2.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#standardization\n",
    "#what is tokenization ? Tokenization is a process of breaking a stream of text up into words phrases symbols or a meaningful elements\n",
    "def tokenize_words(input):\n",
    "    #lowercase everything  to standardize it\n",
    "    input = input.lower()\n",
    "    #instanting the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizing the text into tokens\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    #filtering the stopwords using  lambda\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'),tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "#preprocess the input data, make tokens\n",
    "processed_inputs = tokenize_words(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars to numbers\n",
    "#convert characters in our input to numbers\n",
    "#we'll sort the list of the set of all characters that appear in out i/p text and then use the enumerate fn to get numbers to that\n",
    "#represent the characters\n",
    "# we'll then create a dictionary that stores the keys and values, or the characters and the numbers that represent them\n",
    "chars =  sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269995\n",
      "Total vocab: 43\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars or chars to num (71) has worked?\n",
    "#just so we get an idea of whether our process of converting words to characters has worked, we print the length of our variables\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seg length\n",
    "#we're defining how long we want an indivisual sequence here\n",
    "#in indivisual sequence is a complete mapping of input characters as integers\n",
    "seq_length =100\n",
    "x_data =[]\n",
    "y_data =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269895\n"
     ]
    }
   ],
   "source": [
    "#loop through the sequence\n",
    "#here we're going through the entire list of i/ps and converting the chars  to numbers with a for loop\n",
    "#this will create a bunch of sequence where each sequence starts with next characters in the i/p data beginning with the first char\n",
    "for i in range(0, input_len - seq_length,1):\n",
    "    #define i/p and o/p sequences\n",
    "    #i/p is the current char plus the desired sequence length\n",
    "    in_seq = processed_inputs[i:i+seq_length]\n",
    "    #out sequences is the initial characters plus total sequence length\n",
    "    out_seq = processed_inputs[i+seq_length]\n",
    "    #converting the list of characters to integers based on previous values and appending the values to our lists \n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "#check to see how many total input sequences we have   \n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input sequence to np array that our network can use\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding our label data\n",
    "y= np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "#creating a sequential model\n",
    "#dropout is used to prevent overfitting\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences= True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences= True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights\n",
    "filepath=\"model_weights_saved.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.9139\n",
      "Epoch 00001: loss improved from inf to 2.91389, saving model to model_weights_saved.hdf5\n",
      "1055/1055 [==============================] - 168s 160ms/step - loss: 2.9139\n",
      "Epoch 2/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.6247\n",
      "Epoch 00002: loss improved from 2.91389 to 2.62468, saving model to model_weights_saved.hdf5\n",
      "1055/1055 [==============================] - 168s 159ms/step - loss: 2.6247\n",
      "Epoch 3/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.4752\n",
      "Epoch 00003: loss improved from 2.62468 to 2.47524, saving model to model_weights_saved.hdf5\n",
      "1055/1055 [==============================] - 168s 160ms/step - loss: 2.4752\n",
      "Epoch 4/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.3539\n",
      "Epoch 00004: loss improved from 2.47524 to 2.35393, saving model to model_weights_saved.hdf5\n",
      "1055/1055 [==============================] - 168s 159ms/step - loss: 2.3539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc55019b320>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model and let it train\n",
    "model.fit(X,y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of the model back into characters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: \n",
      "\" eserts scotland endured incalculable fatigue cold hunger dare destroy hopes begone break promise nev \"\n"
     ]
    }
   ],
   "source": [
    "#random seed to help gererate \n",
    "start= numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed: \")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare s"
     ]
    }
   ],
   "source": [
    "#generate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x= x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern= pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
